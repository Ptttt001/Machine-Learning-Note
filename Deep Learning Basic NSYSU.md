# activate function
## why do we need activate function
若不使用像RELU、sigmoid 這類的activate function則層與層之間仍是做現性轉換、如旋轉等、仍無法轉換資料的分布
# backorioagation
## 只可意會不可言傳
第50頁爲什麼是負的
會考computaional graph如同64頁
加法的local gradient是1
82頁有各種gradient計算
